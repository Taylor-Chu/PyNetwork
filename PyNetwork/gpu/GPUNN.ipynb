{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopencl as cl\n",
    "from pyopencl.elementwise import ElementwiseKernel\n",
    "import pyopencl.array as cl_array\n",
    "import pyopencl.reduction as cl_reduction\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from elementwise import *\n",
    "from c_code import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUOPERATOR:\n",
    "    \"\"\"\n",
    "    GPU operators with return values still stored on GPU. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, context, queue):\n",
    "        self.context = context\n",
    "        self.queue = queue\n",
    "        self.program = cl.Program(self.context, c_code).build()\n",
    "\n",
    "    def add(self, A, B):\n",
    "        \"\"\"A, B are assumed to be on the device\"\"\"\n",
    "        height, width = A.shape \n",
    "        heightB, widthB = B.shape    \n",
    "        assert height == heightB and width == widthB, \"Arrays have different shapes.\"\n",
    "        out = cl_array.zeros_like(A)\n",
    "\n",
    "        self.program.ew_add(self.queue, (width, height), None, \n",
    "                            A.data, B.data, np.int32(width), out.data).wait()\n",
    "        return out\n",
    "        \n",
    "    def sub(self, A, B):\n",
    "        \"\"\"A, B are assumed to be on the device\"\"\"\n",
    "        height, width = A.shape \n",
    "        heightB, widthB = B.shape    \n",
    "        assert height == heightB and width == widthB, \"Arrays have different shapes.\"\n",
    "        out = cl_array.zeros_like(A)\n",
    "\n",
    "        self.program.ew_sub(self.queue, (width, height), None, \n",
    "                            A.data, B.data, np.int32(width), out.data).wait()\n",
    "        return out\n",
    "\n",
    "    def div(self, A, B):\n",
    "        \"\"\"A, B are assumed to be on the device\"\"\"\n",
    "        height, width = A.shape \n",
    "        heightB, widthB = B.shape    \n",
    "        assert height == heightB and width == widthB, \"Arrays have different shapes.\"\n",
    "        out = cl_array.zeros_like(A)\n",
    "\n",
    "        self.program.ew_div(self.queue, (width, height), None, \n",
    "                            A.data, B.data, np.int32(width), out.data).wait()\n",
    "        return out\n",
    "\n",
    "    def mul(self, A, B):\n",
    "        \"\"\"A, B are assumed to be on the device\"\"\"\n",
    "        height, width = A.shape \n",
    "        heightB, widthB = B.shape    \n",
    "        assert height == heightB and width == widthB, \"Arrays have different shapes.\"\n",
    "        out = cl_array.zeros_like(A)\n",
    "\n",
    "        self.program.ew_mul(self.queue, (width, height), None, \n",
    "                            A.data, B.data, np.int32(width), out.data).wait()\n",
    "        return out\n",
    "\n",
    "    def greater(self, A, B):\n",
    "        \"\"\"A, B are assumed to be on the device\"\"\"\n",
    "        height, width = A.shape \n",
    "        heightB, widthB = B.shape    \n",
    "        assert height == heightB and width == widthB, \"Arrays have different shapes.\"\n",
    "        out = cl_array.zeros_like(A)\n",
    "\n",
    "        self.program.ew_greater(self.queue, (width, height), None, \n",
    "                               A.data, B.data, np.int32(width), out.data).wait()\n",
    "        return out\n",
    "\n",
    "    def equal(self, A, B):\n",
    "        \"\"\"A, B are assumed to be on the device\"\"\"\n",
    "        height, width = A.shape \n",
    "        heightB, widthB = B.shape    \n",
    "        assert height == heightB and width == widthB, \"Arrays have different shapes.\"\n",
    "        out = cl_array.zeros_like(A)\n",
    "\n",
    "        self.program.ew_equal(self.queue, (width, height), None, \n",
    "                               A.data, B.data, np.int32(width), out.data).wait()\n",
    "        return out\n",
    "\n",
    "    def matmul(self, A, B):\n",
    "        \"\"\"A, B are assumed to be on the device\"\"\"\n",
    "        heightA, widthA = A.shape\n",
    "        heightB, widthB = B.shape\n",
    "        assert widthA == heightB, \"Cannot do matrix multiplication.\"\n",
    "        C = np.empty((heightA, widthB), dtype=np.float32)\n",
    "        out = cl_array.to_device(self.queue, C)\n",
    "\n",
    "        BLOCK_SIZE = 16\n",
    "        local_size = (BLOCK_SIZE, BLOCK_SIZE)\n",
    "\n",
    "        self.program.matrixmultiply2dlocal(self.queue, (heightA, widthB), local_size, \n",
    "                    np.int32(heightA), np.int32(widthA), np.int32(heightB), np.int32(widthB), A.data, B.data, out.data).wait()\n",
    "        return out\n",
    "    \n",
    "    def transpose(self, A):\n",
    "        \"\"\"A is assumed to be on the device\"\"\"\n",
    "        global_size = A.shape\n",
    "        local_size = (16, 16)\n",
    "\n",
    "        width, height = A.shape\n",
    "\n",
    "        A_transpose = cl_array.zeros_like(A)\n",
    "        a_local = cl.LocalMemory(4 * 16 * (16 + 1))\n",
    "        \n",
    "        self.program.transpose(self.queue, global_size, local_size,\n",
    "                               A_transpose.data, A.data, np.int32(width), np.int32(height), a_local).wait()\n",
    "        return A_transpose\n",
    "\n",
    "    def sum(self, A, axis=None):\n",
    "        \"\"\"A is assumed to be on the device\"\"\"\n",
    "        heightA, widthA = A.shape\n",
    "\n",
    "        if axis == None:\n",
    "            rk = cl_reduction.ReductionKernel(self.context, np.float32, neutral=\"0\", reduce_expr=\"a+b\", map_expr=\"a[i]\",\n",
    "                            arguments=\"__global const float *a\")\n",
    "            output_sum = rk(A)\n",
    "            return output_sum        \n",
    "    \n",
    "        else:\n",
    "            if axis == 1:\n",
    "                A = self.transpose(A).copy()\n",
    "            \n",
    "            r = np.empty(A.shape[1]).astype(np.float32)\n",
    "            r_gpu = cl_array.to_device(self.queue, r)\n",
    "            self.program.reduce(self.queue, (A.shape[0]*A.shape[1], ), None, \n",
    "                                A.data, r_gpu.data, np.int32(A.shape[1]), np.int32(A.shape[0])).wait()\n",
    "            return r_gpu\n",
    "    \n",
    "    def exp(self, A):\n",
    "        \"\"\"A is assumed to be on the device\"\"\"\n",
    "        out = cl_array.zeros_like(A)\n",
    "        programme = ElementwiseKernel(self.context,\"float *a_gpu, float *out\",\n",
    "                                          \"out[i] = exp(a_gpu[i])\",\n",
    "                                          \"exponential\")\n",
    "        \n",
    "        programme(A, out)\n",
    "        return out\n",
    "\n",
    "    def log(self, A):\n",
    "        \"\"\"A is assumed to be on the device\"\"\"\n",
    "        out = cl_array.zeros_like(A)\n",
    "        programme = ElementwiseKernel(self.context,\"float *a_gpu, float *out\",\n",
    "                                          \"out[i] = log(a_gpu[i])\",\n",
    "                                          \"logarithm\")\n",
    "        programme(A, out)\n",
    "        return out\n",
    "\n",
    "    def pow(self, A, b):\n",
    "        \"\"\"A is assumed to be on the device\"\"\"\n",
    "        out = cl_array.zeros_like(A)\n",
    "        programme = ElementwiseKernel(self.context,\"float *a_gpu, float b, float *out\",\n",
    "                                          \"out[i] = pow(a_gpu[i],b)\",\n",
    "                                          \"power\")\n",
    "        programme(A, b, out)\n",
    "        return out\n",
    "    \n",
    "    def sqrt(self, A):\n",
    "        \"\"\"A is assumed to be on the device\"\"\"\n",
    "        out = cl_array.zeros_like(A)\n",
    "        programme = ElementwiseKernel(self.context,\"float *a_gpu, float *out\",\n",
    "                                          \"out[i] = sqrt(a_gpu[i])\",\n",
    "                                          \"sqrt\")\n",
    "        programme(A, out)\n",
    "        return out\n",
    "\n",
    "    def abs(self, A):\n",
    "        \"\"\"A is assumed to be on the device\"\"\"\n",
    "        out = cl_array.zeros_like(A)\n",
    "        programme = ElementwiseKernel(self.context,\"float *a_gpu, float *out\",\n",
    "                                          \"out[i] = fabs(a_gpu[i])\",\n",
    "                                          \"abs\")\n",
    "        programme(A, out)\n",
    "        return out\n",
    "\n",
    "    def relu(self, A):\n",
    "        \"\"\"A is assumed to be on the device\"\"\"\n",
    "        out = cl_array.zeros_like(A)\n",
    "        programme = ElementwiseKernel(self.context,\"float *a_gpu, float *out\",\n",
    "                                          \"out[i] = 0.5* (a_gpu[i] + fabs(a_gpu[i]))\",\n",
    "                                          \"relu\")\n",
    "        programme(A, out)\n",
    "        return out\n",
    "    \n",
    "    def tanh(self, A):\n",
    "        \"\"\"A is assumed to be on the device\"\"\"\n",
    "        out = cl_array.zeros_like(A)\n",
    "        programme = ElementwiseKernel(self.context,\"float *a_gpu, float *out\",\n",
    "                                          \"out[i] = tanh(a_gpu[i])\",\n",
    "                                          \"tanh\")\n",
    "        programme(A, out)\n",
    "        return out\n",
    "\n",
    "    def sigmoid(self, A):\n",
    "        \"\"\"A is assumed to be on the device\"\"\"\n",
    "        out = cl_array.zeros_like(A)\n",
    "        programme = ElementwiseKernel(self.context,\n",
    "                                    \"float *x, float *out\",\n",
    "                                    \"out[i] = SIGMOID(x[i])\",\n",
    "                                    \"sigmoid\",\n",
    "                                    preamble='#define SIGMOID(x) x > 0 ? 1.0/(1.0 + exp(-x)) : exp(x) / (exp(x) + 1.0)'\n",
    "                                    )\n",
    "        programme(A, out)\n",
    "        return out\n",
    "    \n",
    "    def swish(self, A):\n",
    "        \"\"\"A is assumed to be on the device\"\"\"\n",
    "        out = cl_array.zeros_like(A)\n",
    "        programme = ElementwiseKernel(self.context,\n",
    "                                    \"float *x, float *out\",\n",
    "                                    \"out[i] = SWISH(x[i])\",\n",
    "                                    \"swish\",\n",
    "                                    preamble='#define SWISH(x) x > 0 ? x/(1.0 + exp(-x)) : x*exp(x) / (exp(x) + 1.0)'\n",
    "                                    )\n",
    "        programme(A, out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = cl.get_platforms()\n",
    "devices = platform[0].get_devices()\n",
    "context = cl.Context(devices)\n",
    "queue = cl.CommandQueue(context)\n",
    "\n",
    "operator = GPUOPERATOR(context, queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tests of the GPU operators\n",
    "m, n= 2**8, 2**8\n",
    "A = np.random.rand(m, n).astype(np.float32)\n",
    "B = np.random.rand(m, n).astype(np.float32)\n",
    "\n",
    "C = cl_array.to_device(queue, A)\n",
    "D = cl_array.to_device(queue, B)\n",
    "\n",
    "add_AB = operator.add(C, D)\n",
    "matmal_AB = operator.matmul(C, D)\n",
    "power_3 = operator.pow(C, 3)\n",
    "\n",
    "sum0 = operator.sum(C, axis=0) \n",
    "\n",
    "np.testing.assert_almost_equal(add_AB.get(), A+B, decimal=3)\n",
    "np.testing.assert_almost_equal(matmal_AB.get(), A@B, decimal=2)\n",
    "np.testing.assert_almost_equal(power_3.get(), A**3, decimal=2)\n",
    "np.testing.assert_almost_equal(sum0.get(), np.sum(A, axis=0), decimal=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HPC4M",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
